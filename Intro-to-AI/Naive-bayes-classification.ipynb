{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Assignment 3 - Part II Naive Bayes Classification (5 points) #\n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the supporting material for topics covered in **Chapter 13 Quantifying Uncertainty**and **Chapter 20 - Statistical Learning Method** from the book *Artificial Intelligence: A Modern Approach.* This part does NOT rely on the provided code so you can complete it just using basic Python. \n",
    "\n",
    "```\n",
    "Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.\n",
    "\n",
    "Gould, Stephen Jay\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of categories, on the basis of text it contains. Naive Bayes models are often used for this task. In these models, the query variable is\n",
    "the document category, and the effect variables are the presence/absence\n",
    "of each word in the language; the assumption is that words occur independently in documents within a given category (condititional independence), with frequencies determined by document category. Download the following file: http://www.cs.cornell.edu/People/pabo/movie-review-data/review_polarity.tar.gz containing a dataset that has been used for text mining consisting of movie reviews classified into negative and positive. You\n",
    "will see that there are two folders for the positivie and negative category and they each contain multiple text files with the reviews. You can find more information about the dataset at: \n",
    "http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "\n",
    "Our goal will be to build a simple Naive Bayes classifier for this dataset. More complicated approaches using term frequency and inverse document frequency weighting and many more words are possible but the basic concepts\n",
    "are the same. The goal is to understand the whole process so DO NOT use existing machine learning packages but rather build the classifier from scratch.\n",
    "\n",
    "Our feature vector representation for each text file will be simply a binary vector that shows which of the following words are present in the text file: Awful Bad Boring Dull Effective Enjoyable Great Hilarious. For example the text file cv996 11592.txt would be represented as (0, 0, 0, 0, 1, 0, 1, 0) because it contains Effective and Great but none of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2A (Minimum) CSC421 -  (1 point, CSC581C - 0 points) \n",
    "\n",
    "Write code that parses the text files and calculates the probabilities for\n",
    "each dictionary word given the review polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Reviews:\n",
      "--------------------\n",
      "awful          1.9%\n",
      "bad           25.4%\n",
      "boring         4.8%\n",
      "dull           2.3%\n",
      "effective     12.0%\n",
      "enjoyable      9.4%\n",
      "great         40.5%\n",
      "hilarious     12.5%\n",
      "\n",
      "Negative Reviews:\n",
      "--------------------\n",
      "awful          9.9%\n",
      "bad           50.3%\n",
      "boring        16.6%\n",
      "dull           9.0%\n",
      "effective      4.6%\n",
      "enjoyable      5.3%\n",
      "great         28.2%\n",
      "hilarious      4.8%\n"
     ]
    }
   ],
   "source": [
    "#Jonathan Kalmar V00762777\n",
    "import os\n",
    "\n",
    "#NOTE: this program assumes that the current working directory contains the folder\n",
    "#'review_polarity', containing the subfolders with positive and negative movie reviews.\n",
    "\n",
    "def get_representation(filename, keywords):\n",
    "    #takes a text file and returns a list representing word presence in the form\n",
    "    #[0,0,0,0,0,0,0,0]\n",
    "    f = open(filename, 'r')\n",
    "    rep=[0,0,0,0,0,0,0,0]\n",
    "    text_words = f.read().split(' ')\n",
    "    for i in range(0, len(keywords)):\n",
    "        if keywords[i] in text_words:\n",
    "            rep[i]=1\n",
    "        else:\n",
    "            rep[i]=0\n",
    "    return rep\n",
    "\n",
    "def mass_representation(folder,keywords):\n",
    "    #takes a folder directory containing text files of movie reviews, and a keyword list\n",
    "    #returns a dictionary where keys are reviews numbered 1-1000, and their values are\n",
    "    #the associated binary word representation in form [0,0,0,0,0,0,0,0,0]\n",
    "    feature_vector = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        rep = get_representation(folder + '/' + filename,keywords)\n",
    "        feature_vector[int(filename[2:5])]=rep\n",
    "    return feature_vector\n",
    "\n",
    "def probabilities(data, keywords):\n",
    "    #takes a dictionary as created by mass_representation, and returns an array\n",
    "    #with probabilities for keyword occurence in the given dataset.\n",
    "    probabilities=[0 for i in range(len(keywords))]\n",
    "    for i in range(0,len(keywords)):\n",
    "        total = 0\n",
    "        for key in data:\n",
    "            if data[key][i]==1: total += 1\n",
    "        probabilities[i]=total/1000\n",
    "    return probabilities\n",
    "        \n",
    "key_words = ('awful','bad','boring','dull','effective','enjoyable','great','hilarious')\n",
    "filepath = os.getcwd() + '/review_polarity/txt_sentoken/'\n",
    "positive_reviews = mass_representation(filepath + 'pos', key_words)\n",
    "negative_reviews = mass_representation(filepath + 'neg', key_words)\n",
    "word_probs_pos = probabilities(positive_reviews, key_words)\n",
    "word_probs_neg = probabilities(negative_reviews, key_words)\n",
    "\n",
    "print('Positive Reviews:')\n",
    "print('-'*20)\n",
    "for i, probability in enumerate(word_probs_pos):\n",
    "    print('{:<10s}{:>8.1f}%'.format(key_words[i],probability*100))\n",
    "\n",
    "print('\\nNegative Reviews:')\n",
    "print('-'*20)\n",
    "for i, probability in enumerate(word_probs_neg):\n",
    "    print('{:<10s}{:>8.1f}%'.format(key_words[i],probability*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2B (Minimum) (CSC421 - 1 point, CSC581C - 0 point) \n",
    "\n",
    "\n",
    "Explain how the probability estimates for each dictionary word given the review polarity can be combined to form a Naive Bayes classifier. You can look up Bernoulli Bayes model for this simple model where only presence/absence of a word is modeled.\n",
    "\n",
    "Your answer should be a description of the process with equations and a specific example as markdown text NOT python code. You will write the code in the next questinon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ANSWER\n",
    "We can find the chance that a review is positive by taking the occurrence of each of our keywords in the review (0 or 1), taking the probability of that word occurring (or not occurring, in which case use 1 minus the probability of it occuring) in a positive review, as calculated above, and multiplying the probabilities together. The probability of each word occurring is assumed to be independent so we do not have to worry about conditional probabilities. To find the chance that the review is negative, repeat the process but use the probabilities of the words occuring in negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2C (Expected) 1 point \n",
    "\n",
    "Write Python code for classifying a particular test instance (in our case movie review) following a Bernolli Bayes approach. Your code should calculate the likelihood the review is positive given the correspondng conditional probabilities for each dictionary word as well as the likelihood the review is negative given the corresponding conditional probabilities for each dictionary word. Check that your code works by providing a few example cases of prediction. Your code should be written from \"scratch\" and only use numpy/scipy but not machine learning libraries like scikit-learn or tensorflow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "Chance that review #449 is positive: 2.0%\n",
      "Chance that review #449 is negative: 0.5%\n",
      "Prediction: review is positive\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def likelihood(review, word_probs): \n",
    "    #given a binary keyword occurence representation of a movie review, and probabilities\n",
    "    #for occurence of a class of review, gives the probability a review is of that class\n",
    "    probability_product = 1.0 \n",
    "    for (i,w) in enumerate(review):\n",
    "        if (w==1): \n",
    "            probability = word_probs[i]\n",
    "        else: \n",
    "            probability = 1.0 - word_probs[i]\n",
    "        probability_product *= probability\n",
    "    return probability_product \n",
    "\n",
    "def predict(review):\n",
    "    #given a binary keyword occurence representation of a movie review, return probability\n",
    "    #that the review is pos, probability it is negative, and the resulting prediction.\n",
    "    scores = [likelihood(review, word_probs_pos), \n",
    "             likelihood(review, word_probs_neg),\n",
    "             \"unclassified\"]\n",
    "    if scores[0]>scores[1]:\n",
    "        scores[2] = \"positive\"\n",
    "    elif scores[0]<scores[1]:\n",
    "        scores[2] = \"negative\"\n",
    "    return scores\n",
    "\n",
    "i = np.random.randint(1000)\n",
    "random_review = positive_reviews[i]\n",
    "print(random_review)\n",
    "result = predict(random_review)\n",
    "\n",
    "print('Chance that review #{:} is positive: {:0.1f}%'.format(i,result[0]*100))\n",
    "print('Chance that review #{:} is negative: {:0.1f}%'.format(i,result[1]*100))\n",
    "print('Prediction: review is', result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2D (Expected ) 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the classification accuracy and confusion matrix that you would obtain using the whole data set for both training and testing. Do not use machine learning libraries like scikit-learn or tensorflow for this only the basic numpy/scipy stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════╤══════════════════════╤══════════════════════╕\n",
      "│                 │  predicted positive  │  predicted negative  │\n",
      "╞═════════════════╪══════════════════════╪══════════════════════╡\n",
      "│ actual positive │         756          │         244          │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┤\n",
      "│ actual negative │         411          │         589          │\n",
      "╘═════════════════╧══════════════════════╧══════════════════════╛\n",
      "\n",
      "Prediction accuracy: 67.25%\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "\n",
    "#tally up the true positive, true negative, false positive, and false negative predictions\n",
    "for review in positive_reviews:\n",
    "    prediction = predict(positive_reviews[review])[2]\n",
    "    if prediction == \"positive\":\n",
    "        tp += 1\n",
    "    elif prediction == \"negative\":\n",
    "        fn += 1\n",
    "for review in negative_reviews:\n",
    "    prediction = predict(negative_reviews[review])[2]\n",
    "    if prediction == \"positive\":\n",
    "        fp += 1\n",
    "    elif prediction == \"negative\":\n",
    "        tn += 1\n",
    "\n",
    "#print confusion matrix\n",
    "headers = ['','predicted positive','predicted negative']\n",
    "data = [['actual positive', tp, fn],\n",
    "        ['actual negative', fp, tn]]\n",
    "print(tabulate(data, headers, tablefmt=\"fancy_grid\", numalign = \"center\"))\n",
    "\n",
    "#calculate and print accuracy\n",
    "accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "print('\\nPrediction accuracy: {:>0.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2E (Advanced) 1 point \n",
    "\n",
    "One can consider the Naive Bayes classifier a generative model that can generate binary feature vectors using the associated probabilities from the training data. The idea is similar to how we do direct sampling in\n",
    "Bayesian Networks and depends on generating random number from a discrete distribution. Describe how you would generate random movie reviews consisting solely of the words from the dictionary using your model. Show 5 examples of randomly generated positive reviews and 5 examples of randomly generated negative reviews. Each example should consists of a subset of the words in the dictionary. Hint: use probabilities to generate both the presence and absence of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Positive Reviews: \n",
      "-------------------------\n",
      "[]\n",
      "['dull', 'hilarious']\n",
      "[]\n",
      "['bad', 'hilarious']\n",
      "['bad']\n",
      "\n",
      "Random Negative Reviews: \n",
      "-------------------------\n",
      "['bad', 'boring', 'dull']\n",
      "['bad']\n",
      "['awful', 'bad', 'enjoyable']\n",
      "['bad']\n",
      "['hilarious']\n"
     ]
    }
   ],
   "source": [
    "#for each word in our dictionary, generate a random float between 0 and 1 (step size of 0.1 will do)\n",
    "#if the randomly generated number is higher than the probability for that class, then include it\n",
    "#if the generated number is lower than the probability, omit it from the review.\n",
    "\n",
    "print('Random Positive Reviews: ')\n",
    "print('-'*25)\n",
    "for i in range(0,5):\n",
    "    print([w for (i,w) in enumerate(key_words) if word_probs_pos[i] > np.random.randint(0,101)/100])\n",
    "print('\\nRandom Negative Reviews: ')\n",
    "print('-'*25)\n",
    "for i in range(0,5):\n",
    "    print([w for (i,w) in enumerate(key_words) if word_probs_neg[i] > np.random.randint(0,101)/100])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
